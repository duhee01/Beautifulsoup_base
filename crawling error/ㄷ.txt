import re
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from time import sleep 
from datetime import datetime
import pandas as pd
import requests
from bs4 import BeautifulSoup

dates = [] #날짜저장
titles = [] #제목저장
contents = [] #본문저장
i = 1 
k = 1
p = 2

#chrome브라우저 열기

def set_chrome_driver():
    chrome_options = webdriver.ChromeOptions()
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
    return driver

def clean_text(news_text):
    text = re.sub('[-=+,#/\?:^.@*\"※~【▶◀ㆍ!』‘|\(\)\[\]`\'…》\”\“\’·]', ' ', news_text)
    text = text.replace("\n","",1000)
    text = text.replace("// flash 오류를 우회하기 위한 함수 추가 function _flash_removeCallback() {}", "")
    return text

def clean_date(date_text):
    date = datetime.strptime(date_text, '%Y.%m.%d')
    date = date.strftime('%Y-%m-%d')
    return date

driver=set_chrome_driver()

driver.implicitly_wait(10)

webpage = driver.get('https://news.naver.com')#네이버창 열기

driver.maximize_window() #창 최대


url = 'https://news.naver.com/main/main.naver?mode=LSD&mid=shm&sid1=101'
# header 설정 없이 그냥 요청하면 네이버에서 차단합니다. 아마 하도 크롤링하는 사람들이 많아서 막은 거 같습니다.
headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36'}

# 요청 시작
r = requests.get(url, headers=headers)

#'경제' 기사 크롤링
driver.find_element(By.XPATH,'/html/body/section/header/div[2]/div/div/div[1]/div/div/ul/li[3]/a/span').click()
for n in range(1000):
    try:
        page_find = '#paging > a:nth-child('+str(p)+')'
        url =  '//*[@id="section_body"]/ul['+str(i)+"]/li["+str(k)+"]/dl/dt[2]/a"
        driver.find_element(By.XPATH, url).click()
        #날짜
        date_time = driver.find_element(By.XPATH, '//*[@id="ct"]/div[1]/div[3]/div[1]/div/span').text[:10]
        date = clean_date(date_time)
        dates.append(date)
        #print(dates)
        #제목
        title = driver.find_element(By.XPATH, '//*[@id="ct"]/div[1]/div[2]/h2').text[:]
        titles.append(title)
        #print(titles)
        #본문
        text = str(driver.find_element(By.XPATH,'//*[@id="dic_area"]').text)
        news_text = clean_text(text)
        contents.append(news_text)
        #print(contents)
        sleep(1)
        news_df = pd.DataFrame({'title':titles,'date':dates,'content':contents})
        news_df.to_csv('C:\\Users\\user\\OneDrive\\문서\\GitHub\\Beautifulsoup_base\\news\\NaverNews.csv',index=False,encoding='utf-8-sig')
        driver.back()
        print(url)
        if k < 6:
            k += 1

        if k > 5:
            i += 1
            k = 1

        if i == 5:
            page_find = '#paging > a:nth-child('+str(p)+')'
            page_click = driver.find_element(By.CSS_SELECTOR, page_find).click()
            i = 1
            k = 1
            p += 1
            print(p)
            sleep(2)

        if p == 10:
            driver.find_element(By.CSS_SELECTOR, "#paging > a._paging.next.nclicks\(air\.next\)").click()
            p = 3
            sleep(1)


    
    except:
        url = 'https://news.naver.com/main/main.naver?mode=LSD&mid=shm&sid1=101'
        html = requests.get(url).text
        print(html)
        
  

            
            
        
